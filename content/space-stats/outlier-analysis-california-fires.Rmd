---
title: 'Tidy Tuesday Week 21: Statistical Outliers of California Fires'
author: "Corey Pembleton"
date: "August 26, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

bytes <- file.size("outlier-analysis-california-fires.Rmd")
words <- bytes/10
minutes <- words/200

library(tidyverse)
library(lubridate)
library(janitor) 
library(scales)
library(ruler)
```

Reading Time: `r round(minutes)` minutes  

## Determining Statistically Outlying Fires in California

I saw some interesting weekly projects being created for this tidytuesday exercise, so wanted to take the chance to see if and how the increased prevalence of fires in California are statistically significant or not. By following the very useful approach on [question flow](http://www.questionflow.org/2017/12/26/combined-outlier-detection-with-dplyr-and-ruler/) which allows the combination of multiple outlier detection methods to be used in measuring quantitative statistical outliers. This approach follows tidy principles and uses the ```ruler``` package, which makes it highly intutive to interpret results and make adjustments to test for robustness of outliers, and include multiple outlier measurement methods. 

The first thing, loading and joining the datasets from the tidytuesday github page:

```{r, message=FALSE, warning=FALSE, cache=TRUE}
#load the data into a tibble
fire_incidents <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018-08-21/cal-fire-incidents.csv")

fire_damage <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018-08-21/calfire_damage.csv")

```

```{r, message=FALSE, warning=FALSE,}
fire_data <- fire_damage %>% 
  left_join(fire_incidents, by = c("year" = "YEAR")) %>% 
  select(year, structures, "number_of_fires" = "NUMBER OF FIRES",
         "acres_burned" = "ACRES BURNED", "dollar_damage" = "DOLLAR DAMAGE")

```

The first step is determining what values will NOT be considered as outliers by three rules:

### Outlier Rule 1: Z-score Threshold

```{r}
#function creation----
#define what isn't an outlier based on z-score
#"Observation is not an outlier based on z-score if its absolute value of default 
#z-score is lower then some threshold '

not_out_z <- function(x, thres = 3, na.rm =TRUE) {
  abs(x - mean(x, na.rm = na.rm)) <= thres * sd(x, na.rm = na.rm) 
}

```

### Outlier Rule 2: Median Absolute Deviation (MAD) threshold

This rule dictates: 
> "*If the observation is not an outlier based on MAD if its absolute value of z-score with median as center and MAD as normalization unit is lower then some threshold (popular choice is 3)*."

```{r}
not_out_mad <- function(x, thres = 3, na.rm = TRUE){
  abs(x - median(x, na.rm = na.rm)) <= thres * mad(x, na.rm = na.rm)
}
```

### Outlier Rule 3: Tukey's Fences threshold

Using Tukey's fences, values considered not to be outliers must fall within the interquantile range set.

```{r}
not_out_tukey <- function(x, k = 1.5, na.rm = TRUE) {
  quar <- quantile(x, probs = c(0.25, 0.75), na.rm = na.rm)
  iqr <- diff(quar)
  (quar[1] - k * iqr <= x) & (x <= quar[2] + k * iqr)
}
```

### Outlier Rule 4: Mahalanobis Centre Distance
Values considered to fall within the Mahalanobis centre will not be considered to be outliers.

```{r}
maha_dist <- . %>% select_if(is.numeric) %>% 
  mahalanobis(center = colMeans(.), cov = cov(.))

not_out_maha <- function(tbl, not_out_f, ...) {
  tbl %>% maha_dist() %>% not_out_f(...)
}
```


```{r, echo=FALSE}
not_outlier_funs <- funs(
  z = not_out_z,
  mad = not_out_mad,
  tukey = not_out_tukey
)
```

### Creating group "packs" of variables not considered to be outliers

Following the creation of rules, the subsequent stage is labelling from our dataset which values are not considered outliers according to the values relative assignment based on each of the rules. The ```ruler``` package does this through the creation of row or column packs, which is a variation on the "nesting" structure of grouped data within the ```purrr::nest()``` family of functions.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
data_tbl <- fire_data %>% 
  unite(col = "group", year)

compute_group_non_outliers <- . %>% 
  group_by(group) %>% 
  summarise_if(is.numeric, mean) %>% 
  ungroup() %>% 
  mutate_if(is.numeric, not_outlier_funs) %>% 
  select_if(Negate(is.numeric))

row_packs_not_out <- row_packs(
  column = . %>% transmute_if(is.numeric, not_outlier_funs),
  maha = . %>% transmute(maha = maha_dist(.)) %>% 
    transmute_at(vars(maha = maha), not_outlier_funs)
)

group_packs_not_out <- group_packs(
  group = compute_group_non_outliers,
  .group_vars = "group"
)

```

Once we can group our variables according to each of the outlier function, I can produce the final report on which points are statistical outliers, and which are not: 

```{r}
full_report <- data_tbl %>% 
  expose(row_packs_not_out, group_packs_not_out,
         .remove_obeyers = FALSE) %>% 
  get_report()

used_rules <- full_report %>% 
  distinct(pack, rule)
```


Which, in turn, lets me determine who is "breaking" the rule of NOT being an outlier (through filter the logical value):

```{r, message=FALSE, warning=FALSE}
breaker_report <- full_report %>% 
  filter(!(value %in% TRUE))

group_breakers <- breaker_report %>% 
  filter(pack == "group") %>% 
  select(-id) %>% 
  left_join(
    y = data_tbl %>% transmute(var = group, id = 1:n()),
    by = "var"
  ) %>% 
  select(pack, rule, var, id, value)

outliers <- bind_rows(
  breaker_report %>% filter(pack != "group"),
  group_breakers
) %>% 
  select(pack, rule, id)

#we can see how not all group based definitions resulted with outliers
outliers %>% 
  count(pack, rule) %>% 
  filter(pack == "group") %>% 
  print(n = Inf)
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}
#outliers can tell us the outlier rows
outlier_score <- outliers %>% 
  group_by(id) %>% 
  summarise(score = n() / nrow(used_rules))

```

```{r, message=FALSE, warning=FALSE}
#tag outliers as `strong outliers` those with a score of more than 0.2

fire_data_outliers <- fire_data %>% 
  mutate(id = 1:n()) %>% 
  left_join(y = outlier_score, by = "id") %>% 
  mutate(
    score = coalesce(score, 0),
    is_out = if_else(score > 0.3, "Outlier", "Not outlier")
  )

```

Finally, I could determine not just those data points which are outliers, but those which are "extreme outliers":

```{r, message=FALSE, warning=FALSE,}
extreme_outliers <- fire_data_outliers %>% 
  filter(score > .2)
```


And subsequently plot them out: 
```{r, message=FALSE, warning=FALSE, echo=FALSE}
fire_data_outliers %>% 
ggplot(aes_string("year", "structures", colour = "is_out")) +
  geom_point(data = fire_data_outliers, aes(size = structures)) +
  scale_colour_manual(values = c("#AAAAAA", "#004080")) +
  scale_x_continuous(breaks = seq(1989, 2017, by = 2), expand = expand_scale(mult = c(.1, .1))) +
  guides(colour = guide_legend(title = NULL,
                               override.aes = list(size = 4))) +
  labs(title = "Strong outliers illustration by ") +
  theme_minimal(base_family = "Lato") + 
  labs(title = "Strong outliers illustrated by number of \nstructures destroyed in California Fires") +
  ylab("# of structures destroyed") +
  theme(legend.position = "bottom",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10),
        axis.ticks = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        axis.text.x = element_text(size = 10, angle = 45)
  )
```

This study was slightly limited in that the tidytuesday data wasn't of the same level of detail across both datasets. Nonetheless, it paints an important picture when considering the extent of damage, and how things are changing in California. At the time of writing, large swaths of California (and the world's forests) continue to burn, with 2018 lining up to undoubtedly be an outlier year.




